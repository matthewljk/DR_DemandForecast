{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM Model Retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from os.path import join\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT = '/home/sdc/DR_DemandForecast/DemandForecast/'\n",
    "ROOT = '/home/meta/Desktop/DR_DemandForecast/DemandForecast'\n",
    "DES_DIR = f'../model/{timestamp}'\n",
    "\n",
    "if not os.path.exists(DES_DIR):\n",
    "    os.makedirs(DES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_conn():\n",
    "    # Load the environment variables from the .env file\n",
    "    env_file = join(ROOT, 'src', '.env')\n",
    "    load_dotenv(env_file)\n",
    "\n",
    "    # Get the values of host, user, pswd, db, and schema from the environment variables\n",
    "    DBHOST = os.getenv('host')\n",
    "    DBUSER = os.getenv('user')\n",
    "    DBPSWD = os.getenv('pswd')\n",
    "    DBNAME = os.getenv('db')\n",
    "    SCHEMA = 'public'\n",
    "\n",
    "    # Use the values as needed\n",
    "    engine = create_engine(f\"postgresql://{DBUSER}:{DBPSWD}@{DBHOST}/{DBNAME}?options=-csearch_path%3D{SCHEMA}\", echo=False)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dpr(\n",
    "    conn: object,\n",
    "    limit: int=None\n",
    "):\n",
    "    select_query = \"\"\"SELECT \"Date\", \"Period\", \"Demand\", \"TCL\", \"TransmissionLoss\", \"Solar\"\n",
    "    FROM public.\"RealTimeDPR\"\n",
    "    ORDER BY \"Date\" DESC, \"Period\" DESC\n",
    "\"\"\"\n",
    "    if limit:\n",
    "        select_query += f\"    LIMIT {limit}\\n;\"\n",
    "    else:\n",
    "        select_query += \";\"\n",
    "        \n",
    "    # print(select_query)\n",
    "    \n",
    "    dpr = pd.read_sql(select_query, conn)\n",
    "    \n",
    "    dpr.fillna(0, inplace=True)\n",
    "    \n",
    "    dpr[\"TotalDemand\"] = dpr[\"Demand\"] + dpr[\"TCL\"] + dpr[\"TransmissionLoss\"] + dpr[\"Solar\"]\n",
    "    \n",
    "    return dpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dpr(\n",
    "    total_demand: pd.Series,\n",
    "    newest_dir: str\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    ## Process Total Demand\n",
    "    \n",
    "    `newest_dir` is needed for loading the latest scaler file.\n",
    "    '''\n",
    "    \n",
    "    total_demand = total_demand.copy()\n",
    "\n",
    "    ''' Scale '''\n",
    "    scaler_files = glob.glob(os.path.join(newest_dir, \"*.pkl\"))\n",
    "    scaler = joblib.load(scaler_files[0])\n",
    "    total_demand = scaler.fit_transform(total_demand.values.reshape(-1,1))\n",
    "\n",
    "    ''' Split '''\n",
    "\n",
    "    weeks = 0.5\n",
    "    days = 0\n",
    "    periods = 0\n",
    "    \n",
    "    if len(total_demand) < 1:\n",
    "        raise ValueError(\"Dataset is empty.\")\n",
    "    \n",
    "    # 1 <len_x> + 1 Y is 1 unit of data.\n",
    "    # There should be at least 6 units of data. This is call a group.\n",
    "    #   - 3 for training, 1 for validation, 2 for test.\n",
    "    \n",
    "    len_x = int(periods + (days * 48) + (weeks * 7 * 48))\n",
    "    # print(len(total_demand), len_x)\n",
    "    \n",
    "    train_units = 2\n",
    "    valid_units = 1\n",
    "    test_units = 1\n",
    "    \n",
    "    if len(total_demand) < (len_x + 1) * (test_units + valid_units + train_units):\n",
    "        raise ValueError(f\"Not enough data for training. {len(total_demand)} < {(len_x + 1) * 6}\")\n",
    "    \n",
    "    max_groups = len(total_demand) // ((len_x + 1) * (test_units + valid_units + train_units))\n",
    "    \n",
    "    idx_test = - max_groups * test_units * (len_x + 1)\n",
    "    idx_vali = - max_groups * (test_units + valid_units) * (len_x + 1)\n",
    "    # print(idx_test, idx_vali)\n",
    "    \n",
    "    test  = total_demand[idx_test:]\n",
    "    valid = total_demand[idx_vali:idx_test]\n",
    "    train = total_demand[:idx_vali]\n",
    "                        \n",
    "    print(f\"train: {len(train)}, validate: {len(valid)}, test: {len(test)}\")\n",
    "    \n",
    "    \n",
    "    ''' Reshape '''\n",
    "    \n",
    "    train_x, train_Y = [], []\n",
    "    for i in range(len(train)-len_x):\n",
    "        train_x.append(train[i:(i+len_x)])\n",
    "        train_Y.extend(train[i + len_x])\n",
    "        \n",
    "    valid_x, valid_Y = [], []\n",
    "    for i in range(len(valid)-len_x):\n",
    "        valid_x.append(valid[i:(i+len_x)])\n",
    "        valid_Y.extend(valid[i + len_x])\n",
    "        \n",
    "    test_x, test_Y = [], []\n",
    "    for i in range(len(test)-len_x):\n",
    "        test_x.append(test[i:(i+len_x)])\n",
    "        test_Y.extend(test[i + len_x])\n",
    "        \n",
    "    return np.array(train_x), np.array(train_Y), np.array(valid_x), np.array(valid_Y), np.array(test_x), np.array(test_Y), scaler\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 13:52:43.051497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-30 13:52:43.051555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-30 13:52:43.053331: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-30 13:52:43.061668: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 13:52:44.059591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as tk\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    newest: str\n",
    "):\n",
    "    \n",
    "    model_files = glob.glob(os.path.join(newest, \"*.keras\"))\n",
    "\n",
    "    # Sort the list of model files by modification time (most recent first)\n",
    "    model_files.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "    # Select the most recent model file\n",
    "    most_recent_model_file = model_files[0]\n",
    "\n",
    "    # Load the selected model\n",
    "    model = tk.models.load_model(most_recent_model_file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "# Load new data\n",
    "conn = db_conn()\n",
    "dpr = get_dpr(\n",
    "    conn, \n",
    "    # limit=336\n",
    ")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 424, validate: 169, test: 169\n"
     ]
    }
   ],
   "source": [
    "# Find newest directory\n",
    "newest_dir = max(glob.glob(os.path.join(ROOT, 'model', '*/')), key=os.path.getmtime)\n",
    "\n",
    "train_x, train_Y, validate_x, validate_Y, test_x, test_Y, scaler = process_dpr(dpr[\"TotalDemand\"], newest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent model\n",
    "model = load_model(newest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Before tuning RMSE: 95.51470312499987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate benchmark predictions and Accuracy\n",
    "test_predictions = model.predict(test_x)\n",
    "\n",
    "i_test_predict = scaler.inverse_transform(test_predictions)\n",
    "i_test_Y = scaler.inverse_transform([test_Y])\n",
    "\n",
    "test_score_p = np.sqrt(mean_squared_error(i_test_Y[0], i_test_predict[:,0]))\n",
    "\n",
    "print(f\"Before tuning RMSE: {test_score_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 14:42:27.792320: I external/local_xla/xla/service/service.cc:168] XLA service 0x7b4058bdf670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-30 14:42:27.792343: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2024-04-30 14:42:27.796494: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714459347.876120 2337070 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "After tuning RMSE: 13.528374999999869\n"
     ]
    }
   ],
   "source": [
    "# Retrain model\n",
    "model.fit(train_x, train_Y, epochs=100, batch_size=32, verbose=0, validation_data=(validate_x, validate_Y))\n",
    "\n",
    "# Generate predictions and Accuracy\n",
    "test_predictions = model.predict(test_x)\n",
    "\n",
    "i_test_predict = scaler.inverse_transform(test_predictions)\n",
    "i_test_Y = scaler.inverse_transform([test_Y])\n",
    "\n",
    "test_score_a = np.sqrt(mean_squared_error(i_test_Y[0], i_test_predict[:,0]))\n",
    "\n",
    "print(f\"After tuning RMSE: {test_score_a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if accuracy is better\n",
    "if test_score_a < test_score_p:\n",
    "    # Define the filename\n",
    "    filename = f'{DES_DIR}/bi-lstm.keras'\n",
    "\n",
    "    # Save the model\n",
    "    model.save(filename)\n",
    "    # Save a report of the model\n",
    "    report = {\n",
    "        'timestamp': timestamp,\n",
    "        'data': {\n",
    "            'lookback': 168,\n",
    "            'weeks': 0.5,\n",
    "            'days': 0,\n",
    "            'periods': 0,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    import json\n",
    "\n",
    "    with open(f'{DES_DIR}/report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp-tf-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
