{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Net Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"postgresql://jason404:Jason404.top@localhost/postgres?options=-csearch_path%3Dsp-df\", echo=True)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from CSV to PostgreSQL\n",
    "\n",
    "Set `IMPORT_DATA` to `True` to import the data from the CSV file to the PostgreSQL database. Set to `False` to skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "if IMPORT_DATA:\n",
    "    \n",
    "    # Load and filer data from csv file\n",
    "    \n",
    "    rt_dpr = pd.read_csv('./data/RT_DPR.csv')\n",
    "    rt_dpr = rt_dpr[['Date', 'Period', 'Demand', 'TCL', 'TransmissionLoss']]\n",
    "    rt_dpr['TransmissionLoss'] = rt_dpr['TransmissionLoss'].fillna(0)\n",
    "    rt_dpr = rt_dpr[rt_dpr['Date'] > '2023-06-30']\n",
    "    rt_dpr = rt_dpr.sort_values(by=['Date', 'Period'])\n",
    "    rt_dpr.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    vc_per = pd.read_csv('./data/VCData_Period.csv')\n",
    "    \n",
    "    rt_dpr.to_sql('RealTime_DPR', conn, if_exists='replace', index=False)\n",
    "    vc_per.to_sql('VCData_Period', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "now = dt.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "time = now.strftime(\"%H:%M\")\n",
    "\n",
    "next_period = int(now.strftime(\"%H\")) * 2 + int(now.strftime(\"%M\")) // 30 + 1\n",
    "print(f\"Net Demand to predict: {date} Period {next_period}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_dpr = pd.read_sql(f\"\"\"\n",
    "                     SELECT \"Date\", \"Period\", \"Demand\", \"TCL\", \"TransmissionLoss\" \n",
    "                     FROM \"RealTime_DPR\" \n",
    "                     WHERE (\"Date\" < '{date}' OR (\"Date\" = '{date}' AND \"Period\" < {next_period}))\n",
    "                     ORDER BY \"Date\" DESC, \"Period\" DESC  \n",
    "                     LIMIT 336\n",
    "                     \"\"\", conn)\n",
    "rt_dpr.sort_values(by=['Date', 'Period'], inplace=True)\n",
    "rt_dpr.reset_index(drop=True, inplace=True)\n",
    "rt_dpr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_dpr.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_per = pd.read_sql('SELECT * FROM \"VCData_Period\"', conn)\n",
    "vc_per.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# Calculate required data fields\n",
    "\n",
    "sg_holidays = holidays.country_holidays('SG')\n",
    "\n",
    "rt_dpr['Total Demand'] = rt_dpr['Demand'] + rt_dpr['TCL'] + rt_dpr['TransmissionLoss']\n",
    "view = rt_dpr[['Date', 'Period', 'Total Demand']].copy()\n",
    "\n",
    "def find_tcq(row):\n",
    "    # print(row)\n",
    "    date_obj = dt.datetime.strptime(row['Date'], '%Y-%m-%d')\n",
    "    year = date_obj.year\n",
    "    quarter = (date_obj.month - 1) // 3 + 1\n",
    "    \n",
    "    isWeekend = 1 if date_obj.isoweekday() > 5 else 0\n",
    "    isPublicHoliday = date_obj in sg_holidays\n",
    "    \n",
    "    if isWeekend or isPublicHoliday:\n",
    "        return vc_per[(vc_per['Year'] == year) & (vc_per['Quarter'] == quarter)]['TCQ_Weekend_PH'].values[0] / 1000\n",
    "    else:\n",
    "        return vc_per[(vc_per['Year'] == year) & (vc_per['Quarter'] == quarter)]['TCQ_Weekday'].values[0] / 1000\n",
    "\n",
    "view['TCQ'] = view.apply(lambda row: find_tcq(row), axis=1)\n",
    "view['Net Demand'] = view['Total Demand'] - view['TCQ']\n",
    "view.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load the most recent scaler file\n",
    "scaler_directory = \"./scaler/\"\n",
    "scaler_files = glob.glob(os.path.join(scaler_directory, \"*.pkl\"))\n",
    "scaler_files.sort(key=os.path.getmtime, reverse=True)\n",
    "scaler = joblib.load(scaler_files[0])\n",
    "print(\"Loaded scaler:\", scaler_files[0])\n",
    "\n",
    "# Perform data preprocessing as before\n",
    "data = view.copy()\n",
    "data['Target'] = data['Net Demand']\n",
    "data['Target'] = scaler.fit_transform(data['Target'].values.reshape(-1,1))\n",
    "\n",
    "# Create dataset for prediction\n",
    "def create_dataset(dataset):\n",
    "    return np.array([dataset])\n",
    "\n",
    "predict_X = create_dataset(data['Target'].values)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "predict_X = np.reshape(predict_X, (predict_X.shape[0], predict_X.shape[1], 1))\n",
    "print(f\"Predict_X shape: {predict_X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from keras.models import load_model\n",
    "\n",
    "# Define the directory where your model files are stored\n",
    "model_directory = \"./model/\"\n",
    "\n",
    "# Get a list of all model files in the directory\n",
    "model_files = glob.glob(os.path.join(model_directory, \"lstm_*.keras\"))\n",
    "\n",
    "# Sort the list of model files by modification time (most recent first)\n",
    "model_files.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "# Select the most recent model file\n",
    "most_recent_model_file = model_files[0]\n",
    "\n",
    "# Load the selected model\n",
    "model = load_model(most_recent_model_file)\n",
    "\n",
    "# Print the path of the loaded model for verification\n",
    "print(\"Loaded model:\", most_recent_model_file)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "predict_result = model.predict(predict_X)\n",
    "\n",
    "# Invert predictions to original scale\n",
    "inverted_predictions = scaler.inverse_transform(predict_result)\n",
    "\n",
    "# Print or use the predictions as needed\n",
    "print(f\"Predictions: {inverted_predictions[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
